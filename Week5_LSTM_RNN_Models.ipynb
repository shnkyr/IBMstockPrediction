{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Data Loading and Preprocessing**"
      ],
      "metadata": {
        "id": "a9eMWzu5O0-R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*1. We will first Load Data*"
      ],
      "metadata": {
        "id": "DqOtxWFOQ8ec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'IBM_2006-01-01_to_2018-01-01.csv'  # Adjust path if necessary\n",
        "ibm_data = pd.read_csv(file_path)\n",
        "\n",
        "# Select relevant columns\n",
        "ibm_data = ibm_data[['Open', 'High', 'Low', 'Close']]\n"
      ],
      "metadata": {
        "id": "5iAwWcZ5O2-Y"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*2: Now we will Normalize Data*"
      ],
      "metadata": {
        "id": "h_Hj4L-lRAFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Min-Max Scaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit and transform the data\n",
        "ibm_scaled = scaler.fit_transform(ibm_data)\n",
        "\n",
        "# Convert to DataFrame for easier handling\n",
        "ibm_scaled = pd.DataFrame(ibm_scaled, columns=ibm_data.columns)\n"
      ],
      "metadata": {
        "id": "zkgGdF-yRF6f"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Performing hyperparameter optimization for  LSTM**"
      ],
      "metadata": {
        "id": "H2Zn7_e-X2ve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U keras-tuner\n",
        "import keras_tuner as kt\n",
        "from tensorflow import keras\n",
        "\n",
        "def build_lstm_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(\n",
        "        units=hp.Int('units', min_value=32, max_value=512, step=32),\n",
        "        return_sequences=True,\n",
        "        input_shape=(X_train.shape[1], X_train.shape[2])\n",
        "    ))\n",
        "    model.add(LSTM(units=hp.Int('units', min_value=32, max_value=50, step=32)))\n",
        "    model.add(Dense(4))\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(\n",
        "            hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')\n",
        "        ),\n",
        "        loss='mean_squared_error'\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Instantiate the tuner\n",
        "tuner = kt.Hyperband(\n",
        "    build_lstm_model,\n",
        "    objective='val_loss',\n",
        "    max_epochs=50,\n",
        "    hyperband_iterations=2,\n",
        "    directory='my_dir',\n",
        "    project_name='keras_lstm'\n",
        ")\n",
        "\n",
        "# Perform hyperparameter tuning\n",
        "tuner.search(X_train, y_train, epochs=10, validation_split=0.1, verbose=1)\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7Si5kVTX_ha",
        "outputId": "c1dd4bcb-1c60-431e-f327-83886e0918ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reloading Tuner from my_dir/keras_lstm/tuner0.json\n",
            "\n",
            "Search: Running Trial #87\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "512               |160               |units\n",
            "0.0071098         |0.0064927         |learning_rate\n",
            "50                |50                |tuner/epochs\n",
            "0                 |17                |tuner/initial_epoch\n",
            "0                 |3                 |tuner/bracket\n",
            "0                 |3                 |tuner/round\n",
            "\n",
            "Epoch 1/50\n",
            "68/68 [==============================] - 33s 396ms/step - loss: 0.2310 - val_loss: 0.0357\n",
            "Epoch 2/50\n",
            "68/68 [==============================] - 25s 371ms/step - loss: 0.1033 - val_loss: 0.1176\n",
            "Epoch 3/50\n",
            "68/68 [==============================] - 37s 539ms/step - loss: 0.0953 - val_loss: 0.0061\n",
            "Epoch 4/50\n",
            "68/68 [==============================] - 25s 371ms/step - loss: 0.1217 - val_loss: 0.0945\n",
            "Epoch 5/50\n",
            "51/68 [=====================>........] - ETA: 6s - loss: 0.1107"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Building LSTM and RNN Models**"
      ],
      "metadata": {
        "id": "70UFsB70SAu_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*1. Preparing Time Series Data for LSTM/RNN Model*"
      ],
      "metadata": {
        "id": "QE9iQje1SB9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_sequences(data, seq_length):\n",
        "    xs = []\n",
        "    ys = []\n",
        "\n",
        "    for i in range(len(data)-seq_length-1):\n",
        "        x = data[i:(i+seq_length)]\n",
        "        y = data[i+seq_length]\n",
        "        xs.append(x)\n",
        "        ys.append(y)\n",
        "\n",
        "    return np.array(xs), np.array(ys)\n",
        "\n",
        "# Define sequence length\n",
        "seq_length = 10  # This is a hyperparameter\n",
        "\n",
        "# Create sequences\n",
        "X, y = create_sequences(ibm_scaled.values, seq_length)\n"
      ],
      "metadata": {
        "id": "LFzcQGRsSGPW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Now we are building the LSTM Model"
      ],
      "metadata": {
        "id": "9gvTPar5SM_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the optimal LSTM model\n",
        "history_optimal_lstm = optimal_lstm_model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=best_hps.get('epochs'),  # The number of epochs to train, as found by the tuner\n",
        "    batch_size=32,  # Or you can use the batch size from the tuner\n",
        "    validation_split=0.1,  # Using part of the training data as validation data\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "CElbaWhPSS6C",
        "outputId": "d794a94d-d4a5-42ff-be59-04e8e1384da4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-f146e540779b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the optimal LSTM model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history_optimal_lstm = optimal_lstm_model.fit(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest_hps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# The number of epochs to train, as found by the tuner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Or you can use the batch size from the tuner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'optimal_lstm_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*3. Now we are building the RNN Model*"
      ],
      "metadata": {
        "id": "cJgj-s8eSXNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import SimpleRNN\n",
        "\n",
        "# Define RNN model\n",
        "rnn_model = Sequential([\n",
        "    SimpleRNN(50, return_sequences=True, input_shape=(X.shape[1], X.shape[2])),\n",
        "    SimpleRNN(50),\n",
        "    Dense(4)  # Predicting 4 values (Open, High, Low, Close)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "rnn_model.compile(optimizer='adam', loss='mean_squared_error')\n"
      ],
      "metadata": {
        "id": "QkOkPB6mSbwc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Training LSTM and RNN Models**"
      ],
      "metadata": {
        "id": "HhH28xaUS9Bf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*1. Splitting Data into Training and Testing Sets*\n",
        "\n",
        "As described in our model strategy"
      ],
      "metadata": {
        "id": "dY-YljFWS-Du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a split for training and testing\n",
        "train_size = int(len(X) * 0.8)\n",
        "test_size = len(X) - train_size\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n"
      ],
      "metadata": {
        "id": "OPKmhuiNTB6m"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*2. Now time for the Training LSTM Model*"
      ],
      "metadata": {
        "id": "Z4Oc_a87TJER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the LSTM model\n",
        "history_lstm = lstm_model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=50,  # Number of epochs is a hyperparameter that can be tuned\n",
        "    batch_size=32,\n",
        "    validation_split=0.1,\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHXBt7nbTMJN",
        "outputId": "3535f268-fdfc-4080-910d-715b962cd0a5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "68/68 [==============================] - 6s 27ms/step - loss: 0.0275 - val_loss: 9.6169e-04\n",
            "Epoch 2/50\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 6.9720e-04 - val_loss: 0.0012\n",
            "Epoch 3/50\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 6.8348e-04 - val_loss: 8.4196e-04\n",
            "Epoch 4/50\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 7.2142e-04 - val_loss: 8.8964e-04\n",
            "Epoch 5/50\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 6.6974e-04 - val_loss: 0.0011\n",
            "Epoch 6/50\n",
            "68/68 [==============================] - 1s 22ms/step - loss: 6.6538e-04 - val_loss: 0.0013\n",
            "Epoch 7/50\n",
            "68/68 [==============================] - 1s 21ms/step - loss: 6.8093e-04 - val_loss: 9.3255e-04\n",
            "Epoch 8/50\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 6.8929e-04 - val_loss: 9.3832e-04\n",
            "Epoch 9/50\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 6.3110e-04 - val_loss: 0.0013\n",
            "Epoch 10/50\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 6.7975e-04 - val_loss: 7.3961e-04\n",
            "Epoch 11/50\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 6.1432e-04 - val_loss: 9.1517e-04\n",
            "Epoch 12/50\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 6.3606e-04 - val_loss: 8.5956e-04\n",
            "Epoch 13/50\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 6.3060e-04 - val_loss: 8.0851e-04\n",
            "Epoch 14/50\n",
            "68/68 [==============================] - 1s 18ms/step - loss: 5.9991e-04 - val_loss: 7.5021e-04\n",
            "Epoch 15/50\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 5.9885e-04 - val_loss: 0.0012\n",
            "Epoch 16/50\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 5.9573e-04 - val_loss: 6.3862e-04\n",
            "Epoch 17/50\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 6.2303e-04 - val_loss: 9.6483e-04\n",
            "Epoch 18/50\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 5.6457e-04 - val_loss: 6.3958e-04\n",
            "Epoch 19/50\n",
            "68/68 [==============================] - 1s 22ms/step - loss: 5.4287e-04 - val_loss: 8.4192e-04\n",
            "Epoch 20/50\n",
            "68/68 [==============================] - 2s 22ms/step - loss: 5.4094e-04 - val_loss: 8.1231e-04\n",
            "Epoch 21/50\n",
            "68/68 [==============================] - 1s 19ms/step - loss: 5.1721e-04 - val_loss: 5.8402e-04\n",
            "Epoch 22/50\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 6.0071e-04 - val_loss: 6.4218e-04\n",
            "Epoch 23/50\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 5.4830e-04 - val_loss: 6.3021e-04\n",
            "Epoch 24/50\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 4.8521e-04 - val_loss: 5.5560e-04\n",
            "Epoch 25/50\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 4.8215e-04 - val_loss: 7.1253e-04\n",
            "Epoch 26/50\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 4.4386e-04 - val_loss: 0.0012\n",
            "Epoch 27/50\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 4.2916e-04 - val_loss: 6.0988e-04\n",
            "Epoch 28/50\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 5.0506e-04 - val_loss: 8.8745e-04\n",
            "Epoch 29/50\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 4.2226e-04 - val_loss: 4.2393e-04\n",
            "Epoch 30/50\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 3.8415e-04 - val_loss: 4.5521e-04\n",
            "Epoch 31/50\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 4.2716e-04 - val_loss: 3.9555e-04\n",
            "Epoch 32/50\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 4.0799e-04 - val_loss: 3.8067e-04\n",
            "Epoch 33/50\n",
            "68/68 [==============================] - 1s 22ms/step - loss: 3.4762e-04 - val_loss: 4.1540e-04\n",
            "Epoch 34/50\n",
            "68/68 [==============================] - 1s 22ms/step - loss: 3.6211e-04 - val_loss: 3.6062e-04\n",
            "Epoch 35/50\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 4.1334e-04 - val_loss: 3.1263e-04\n",
            "Epoch 36/50\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 3.1817e-04 - val_loss: 3.7018e-04\n",
            "Epoch 37/50\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 2.9939e-04 - val_loss: 3.4075e-04\n",
            "Epoch 38/50\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 3.6038e-04 - val_loss: 3.0031e-04\n",
            "Epoch 39/50\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 3.2227e-04 - val_loss: 6.5966e-04\n",
            "Epoch 40/50\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 3.4584e-04 - val_loss: 2.7203e-04\n",
            "Epoch 41/50\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 2.7508e-04 - val_loss: 2.5982e-04\n",
            "Epoch 42/50\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 2.8404e-04 - val_loss: 3.1346e-04\n",
            "Epoch 43/50\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 2.7083e-04 - val_loss: 3.9251e-04\n",
            "Epoch 44/50\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 2.7709e-04 - val_loss: 3.8686e-04\n",
            "Epoch 45/50\n",
            "68/68 [==============================] - 1s 20ms/step - loss: 2.5636e-04 - val_loss: 2.6555e-04\n",
            "Epoch 46/50\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 2.3561e-04 - val_loss: 2.2826e-04\n",
            "Epoch 47/50\n",
            "68/68 [==============================] - 2s 23ms/step - loss: 2.2712e-04 - val_loss: 2.3460e-04\n",
            "Epoch 48/50\n",
            "68/68 [==============================] - 1s 17ms/step - loss: 2.5113e-04 - val_loss: 3.3127e-04\n",
            "Epoch 49/50\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 2.6403e-04 - val_loss: 2.3463e-04\n",
            "Epoch 50/50\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 2.1550e-04 - val_loss: 2.6838e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*3. Now we will start the Training RNN Model*"
      ],
      "metadata": {
        "id": "LKzu8aUfTlza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the RNN model\n",
        "history_rnn = rnn_model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=50,  # Number of epochs can be adjusted\n",
        "    batch_size=32,\n",
        "    validation_split=0.1,\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rR4dnOQMTuAS",
        "outputId": "cd7d683b-2b9a-4f6c-92a1-bbf9280586e1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "68/68 [==============================] - 3s 13ms/step - loss: 0.0115 - val_loss: 0.0014\n",
            "Epoch 2/50\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 8.0759e-04 - val_loss: 7.7581e-04\n",
            "Epoch 3/50\n",
            "68/68 [==============================] - 0s 7ms/step - loss: 6.1812e-04 - val_loss: 5.8218e-04\n",
            "Epoch 4/50\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 5.5225e-04 - val_loss: 4.5217e-04\n",
            "Epoch 5/50\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 5.1251e-04 - val_loss: 7.3816e-04\n",
            "Epoch 6/50\n",
            "68/68 [==============================] - 1s 12ms/step - loss: 4.7896e-04 - val_loss: 5.4089e-04\n",
            "Epoch 7/50\n",
            "68/68 [==============================] - 1s 12ms/step - loss: 3.9142e-04 - val_loss: 3.7427e-04\n",
            "Epoch 8/50\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 3.7837e-04 - val_loss: 3.0806e-04\n",
            "Epoch 9/50\n",
            "68/68 [==============================] - 1s 13ms/step - loss: 4.4006e-04 - val_loss: 4.7957e-04\n",
            "Epoch 10/50\n",
            "68/68 [==============================] - 1s 15ms/step - loss: 4.4268e-04 - val_loss: 3.6695e-04\n",
            "Epoch 11/50\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 3.2405e-04 - val_loss: 3.2352e-04\n",
            "Epoch 12/50\n",
            "68/68 [==============================] - 1s 14ms/step - loss: 3.2376e-04 - val_loss: 3.2941e-04\n",
            "Epoch 13/50\n",
            "68/68 [==============================] - 1s 12ms/step - loss: 3.2338e-04 - val_loss: 4.1316e-04\n",
            "Epoch 14/50\n",
            "68/68 [==============================] - 1s 12ms/step - loss: 3.5157e-04 - val_loss: 5.9244e-04\n",
            "Epoch 15/50\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 2.7302e-04 - val_loss: 2.6900e-04\n",
            "Epoch 16/50\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 2.8563e-04 - val_loss: 6.9859e-04\n",
            "Epoch 17/50\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 2.9678e-04 - val_loss: 5.1386e-04\n",
            "Epoch 18/50\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 2.5950e-04 - val_loss: 2.9751e-04\n",
            "Epoch 19/50\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 2.9418e-04 - val_loss: 3.6881e-04\n",
            "Epoch 20/50\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 2.2848e-04 - val_loss: 2.4301e-04\n",
            "Epoch 21/50\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 2.7711e-04 - val_loss: 6.9578e-04\n",
            "Epoch 22/50\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 2.6020e-04 - val_loss: 3.0357e-04\n",
            "Epoch 23/50\n",
            "68/68 [==============================] - 1s 7ms/step - loss: 2.2625e-04 - val_loss: 4.1947e-04\n",
            "Epoch 24/50\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 2.2628e-04 - val_loss: 3.0792e-04\n",
            "Epoch 25/50\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 2.0619e-04 - val_loss: 2.5326e-04\n",
            "Epoch 26/50\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 2.4529e-04 - val_loss: 1.8675e-04\n",
            "Epoch 27/50\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 2.2324e-04 - val_loss: 2.2742e-04\n",
            "Epoch 28/50\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 2.1384e-04 - val_loss: 3.6784e-04\n",
            "Epoch 29/50\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 2.6553e-04 - val_loss: 2.3436e-04\n",
            "Epoch 30/50\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 2.2703e-04 - val_loss: 3.1816e-04\n",
            "Epoch 31/50\n",
            "68/68 [==============================] - 1s 7ms/step - loss: 1.9570e-04 - val_loss: 3.6175e-04\n",
            "Epoch 32/50\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 2.1376e-04 - val_loss: 2.6000e-04\n",
            "Epoch 33/50\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 1.9983e-04 - val_loss: 2.3789e-04\n",
            "Epoch 34/50\n",
            "68/68 [==============================] - 1s 12ms/step - loss: 2.3159e-04 - val_loss: 3.2823e-04\n",
            "Epoch 35/50\n",
            "68/68 [==============================] - 1s 12ms/step - loss: 2.1083e-04 - val_loss: 1.7749e-04\n",
            "Epoch 36/50\n",
            "68/68 [==============================] - 1s 12ms/step - loss: 2.1382e-04 - val_loss: 1.8884e-04\n",
            "Epoch 37/50\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 1.8957e-04 - val_loss: 1.7368e-04\n",
            "Epoch 38/50\n",
            "68/68 [==============================] - 1s 12ms/step - loss: 2.0041e-04 - val_loss: 2.0799e-04\n",
            "Epoch 39/50\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 2.0500e-04 - val_loss: 1.9916e-04\n",
            "Epoch 40/50\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 1.8364e-04 - val_loss: 1.9311e-04\n",
            "Epoch 41/50\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 2.1148e-04 - val_loss: 1.7954e-04\n",
            "Epoch 42/50\n",
            "68/68 [==============================] - 1s 7ms/step - loss: 1.7400e-04 - val_loss: 3.4920e-04\n",
            "Epoch 43/50\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 1.7300e-04 - val_loss: 1.8027e-04\n",
            "Epoch 44/50\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 1.7865e-04 - val_loss: 2.3809e-04\n",
            "Epoch 45/50\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 2.0359e-04 - val_loss: 1.8891e-04\n",
            "Epoch 46/50\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 2.3532e-04 - val_loss: 4.5090e-04\n",
            "Epoch 47/50\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 2.0076e-04 - val_loss: 2.4700e-04\n",
            "Epoch 48/50\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 1.9326e-04 - val_loss: 3.0960e-04\n",
            "Epoch 49/50\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 2.0511e-04 - val_loss: 1.8224e-04\n",
            "Epoch 50/50\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 2.0643e-04 - val_loss: 3.1517e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Evaluating and Plotting Model Performance**"
      ],
      "metadata": {
        "id": "JksQHn36UUcJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*1. Plotting Training and Validation Loss*\n",
        "\n",
        "After the training of the model, now we will try to plot the results"
      ],
      "metadata": {
        "id": "BKMkUagZUXdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to plot history\n",
        "def plot_history(history, title):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Plot for LSTM\n",
        "plot_history(history_lstm, 'LSTM Training and Validation Loss')\n",
        "\n",
        "# Plot for RNN\n",
        "plot_history(history_rnn, 'RNN Training and Validation Loss')\n"
      ],
      "metadata": {
        "id": "EngQyT9CUju_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}